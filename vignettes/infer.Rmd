---
title: "infer"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{infer}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r include=FALSE}
knitr::opts_chunk$set(fig.width = 6, fig.height = 3.5) 
options(digits = 4)
```

### Introduction

`infer` implements an expressive grammar to perform statistical inference that coheres with the `tidyverse` design framework. Rather than providing methods for specific statistical tests, this package consolidates the principles that are shared among common hypothesis tests into a set of 4 main verbs (functions,) supplemented with many utilities to visualize and extract value from their outputs.

Regardless of which hypothesis test we're using, we're still asking the same kind of question: is the effect/difference in our observed data real, or due to chance? To answer this question, we start by assuming that the observed data came from some world where "nothing is going on" (i.e. the observed effect was simply due to random chance), and call this assumption our *null hypothesis*. (In reality, we might not believe in the null hypothesis at all---the null hypothesis is in opposition to the *alternate hypothesis*, which supposes that the effect present in the observed data is actually due to the fact that "something is going on.") We then calculate a *test statistic* from our data that describes the observed effect. We can use this test statistic to calculate a *p-value*, giving the probability that our observed data could come about if the null hypothesis was true. If this probability is below some pre-defined *significance level* $\alpha$, then we can reject our null hypothesis.

The workflow of this package is designed around this idea. Starting out with some dataset,

+ `specify()` allows you to specify the variable, or relationship between variables, that you're interested in.
+ `hypothesize()` allows you to declare the null hypothesis.
+ `generate()` allows you to construct a null distribution based on your null hypothesis.
+ `calculate()` allows you to calculate a summary statistic describing the effect observed in the data.

Throughout this vignette, we make use of `gss`, a dataset supplied by `infer` containing a sample of 3000 observations sampled from the *General Social Survey*. 

```{r load-devtools, echo = FALSE, message = FALSE, warning = FALSE}
devtools::load_all()
```


```{r load-gss}
# load in the dataset
data(gss)

# take a look at its structure
str(gss)
```

Each row is an individual survey response, containing some basic demographic information on the respondent as well as some additional variables. See `?gss` for more information on the variables included and their source. Note that this data (and our examples on it) are for demonstration purposes only, and will not provide accurate estimates unless weighted properly.

### `specify()`: Specifying Response (and Explanatory) Variables

The `specify` function can be used to specify which of the variables in the dataset you're interested in. If you're only interested in, say, the `age` of the respondents, you might write:

```{r specify-example, warning = FALSE, message = FALSE}
gss %>%
  specify(response = age)
```

On the front-end, the output of `specify` just looks like it selects off the columns in the dataframe that you've specified. Checking the class of this object, though:

```{r specify-one, warning = FALSE, message = FALSE}
gss %>%
  specify(response = age) %>%
  class()
```

We can see that the `infer` class has been appended on top of the dataframe classes--this new class stores some extra metadata.

If you're interested in two variables--`age` and `partyid`, for example--you can `specify` their relationship in one of two (equivalent) ways:

```{r specify-two, warning = FALSE, message = FALSE}
# with the named arguments
gss %>%
  specify(age ~ partyid)

# as a formula
gss %>%
  specify(response = age, explanatory = partyid)
```

If you're doing inference on one proportion or a difference in proportions, you will need to use the `success` argument to specify which level of your `response` variable is a success. For instance, if you're interested in using <!-- need more binary variables, less charged--> , you might use the following code:

```{r specify-success, warning = FALSE, message = FALSE}
# specifying for inference on proportions

```

### `hypothesize()`: Declaring the Null Hypothesis

The next step in the `infer` pipeline is often to declare a null hypothesis using `hypothesize()`. The first step is to supply one of "independence" or "point" to the `null` argument. If your null hypothesis assumes independence between two variables, then this is all you need to supply to `hypothesize()`:

```{r hypothesize-independence, warning = FALSE, message = FALSE}
gss %>%
  specify(sex ~ partyid, success = "female") %>%
  hypothesize(null = "independence")
```

If you're doing inference on a point estimate, you will also need to provide one of `p` (the true proportion of successes, between 0 and 1), `mu` (the true mean), `med` (the true median), or `sigma` (the true standard deviation). For instance, if the null hypothesis is that the mean number of hours worked per week in this sample is 40, we would write:

```{r hypothesize-40-hr-week, warning = FALSE, message = FALSE}
gss %>%
  specify(response = hours) %>%
  hypothesize(null = "point", mu = 40)
```

Again, from the front-end, the dataframe outputted from `hypothesize()` looks almost exactly the same as it did when it came out of `specify()`, but `infer` now "knows" your null hypothesis.

### `generate()`: Generating the Null Distribution

Once we've asserted our null hypothesis using `hypothesize()`, we can construct a null distribution based on this hypothesis. We can do this using one of several methods, supplied in the `type` argument:

* `bootstrap`: A bootstrap sample will be drawn for each replicate, where a sample of size equal to the input sample size is drawn (with replacement) from the input sample data.  
* `permute`: For each replicate, each input value will be randomly reassigned (without replacement) to a new output value in the sample.  
* `simulate`: A value will be sampled from a theoretical distribution with parameters specified in `hypothesize()` for each replicate. (This option is currently only applicable for testing point estimates.)  

Continuing on with our example above, about the average number of hours worked a week, we might write:

```{r generate-point, warning = FALSE, message = FALSE}
gss %>%
  specify(response = hours) %>%
  hypothesize(null = "point", mu = 40) %>%
  generate(reps = 1000, type = "bootstrap")
```

In the above example, we take 1000 bootstrap samples to form our null distribution.

To generate a null distribution for the independence of two variables, we could also randomly reshuffle the pairings of explanatory and response variables to break any existing association. For instance, to generate 1000 replicates that can be used to create a null distribution under the assumption that political party affiliation is not affected by age:

```{r generate-permute, warning = FALSE, message = FALSE}
gss %>%
  specify(partyid ~ age) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute")
```

### `calculate()`: Calculating Summary Statistics

Depending on whether you're carrying out computation-based inference or theory-based inference, you will either supply `calculate()` with the output of `generate()` or `hypothesize`, respectively. The function, for one, takes in a `stat` argument, which is currently one of "mean", "median", "sum", "sd", "prop", "count", "diff in means", "diff in medians", "diff in props", "Chisq", "F", "t", "z", "slope", or "correlation". For example, continuing our example above to calculate the null distribution of mean hours worked per week:

```{r calculate-point, warning = FALSE, message = FALSE}
gss %>%
  specify(response = hours) %>%
  hypothesize(null = "point", mu = 40) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "mean")
```

If you're carrying out inference on differences in means, medians, or proportions, or t and z statistics, you will need to supply an `order` argument, giving the order in which the explanatory variables should be subtracted. For instance, to find the difference in mean age of those that identify as democrat or republican, we might write:

```{r specify-diff-in-means}
gss %>%
  dplyr::filter(partyid %in% c("dem", "rep")) %>%
  specify(age ~ partyid) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate("diff in means", order = c("rep", "dem"))
```

### Other Utilities in infer

<!-- visualize, get_p_value, get_confidence interval --> 

### Examples

We'll work through some examples using `gss`, a dataset supplied by `infer` containing a sample of 3000 observations sampled from the *General Social Survey*. In each, we'll point out the kinds of variable(s) involved, generate a null distribution, calculate the relevant test statistic, visualize where the observed statistic lies in the observed distribution, extract the p-value from the test statistic, and find a confidence interval for the point estimate.

#### Example Template

Point out and describe the relevant variable(s). Mention common terminology that is specific to this use-case.

```{r observed}
# calculate the observed statistic
```

```{r null-dist}
# generate the null distribution
```

```{r test-stat}
# calculate the test statistic
```

```{r visualize}
# visualize where the test statistic lies in the null distribution
```

```{r p-value}
# calculate the p-value from the test statistic
```

```{r confidence-interval}
# find a confidence interval for the point estimate
```
